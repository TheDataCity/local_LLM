{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.llms import LlamaCpp\n",
        "from datetime import datetime\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        ")\n",
        "from constants import CONTEXT_WINDOW_SIZE, MAX_NEW_TOKENS, N_GPU_LAYERS, N_BATCH, MODELS_PATH\n",
        "\n",
        "\n",
        "def load_quantized_model_gguf_ggml(model_id, model_basename, device_type, logging):\n",
        "    \"\"\"\n",
        "    Load a GGUF/GGML quantized model using LlamaCpp.\n",
        "\n",
        "    This function attempts to load a GGUF/GGML quantized model using the LlamaCpp library.\n",
        "    If the model is of type GGML, and newer version of LLAMA-CPP is used which does not support GGML,\n",
        "    it logs a message indicating that LLAMA-CPP has dropped support for GGML.\n",
        "\n",
        "    Parameters:\n",
        "    - model_id (str): The identifier for the model on HuggingFace Hub.\n",
        "    - model_basename (str): The base name of the model file.\n",
        "    - device_type (str): The type of device where the model will run, e.g., 'mps', 'cuda', etc.\n",
        "    - logging (logging.Logger): Logger instance for logging messages.\n",
        "\n",
        "    Returns:\n",
        "    - LlamaCpp: An instance of the LlamaCpp model if successful, otherwise None.\n",
        "\n",
        "    Notes:\n",
        "    - The function uses the `hf_hub_download` function to download the model from the HuggingFace Hub.\n",
        "    - The number of GPU layers is set based on the device type.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        print(f\"{datetime.now()} - Using Llamacpp for GGUF/GGML quantized models\")\n",
        "        model_path = hf_hub_download(\n",
        "            repo_id=model_id,\n",
        "            filename=model_basename,\n",
        "            resume_download=True,\n",
        "            cache_dir=MODELS_PATH,\n",
        "        )\n",
        "        kwargs = {\n",
        "            \"model_path\": model_path,\n",
        "            \"n_ctx\": CONTEXT_WINDOW_SIZE,\n",
        "            \"max_tokens\": MAX_NEW_TOKENS,\n",
        "            \"n_batch\": N_BATCH,  # set this based on your GPU & CPU RAM\n",
        "        }\n",
        "        if device_type.lower() == \"mps\":\n",
        "            kwargs[\"n_gpu_layers\"] = 1\n",
        "        if device_type.lower() == \"cuda\":\n",
        "            kwargs[\"n_gpu_layers\"] = N_GPU_LAYERS  # set this based on your GPU\n",
        "\n",
        "        return LlamaCpp(**kwargs)\n",
        "    except:\n",
        "        if \"ggml\" in model_basename:\n",
        "            print(f\"{datetime.now()} - If you were using GGML model, LLAMA-CPP Dropped Support, Use GGUF Instead\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def load_quantized_model_qptq(model_id, model_basename, device_type, logging):\n",
        "    \"\"\"\n",
        "    Load a GPTQ quantized model using AutoGPTQForCausalLM.\n",
        "\n",
        "    This function loads a quantized model that ends with GPTQ and may have variations\n",
        "    of .no-act.order or .safetensors in their HuggingFace repo.\n",
        "\n",
        "    Parameters:\n",
        "    - model_id (str): The identifier for the model on HuggingFace Hub.\n",
        "    - model_basename (str): The base name of the model file.\n",
        "    - device_type (str): The type of device where the model will run.\n",
        "    - logging (logging.Logger): Logger instance for logging messages.\n",
        "\n",
        "    Returns:\n",
        "    - model (AutoGPTQForCausalLM): The loaded quantized model.\n",
        "    - tokenizer (AutoTokenizer): The tokenizer associated with the model.\n",
        "\n",
        "    Notes:\n",
        "    - The function checks for the \".safetensors\" ending in the model_basename and removes it if present.\n",
        "    \"\"\"\n",
        "\n",
        "    # The code supports all huggingface models that ends with GPTQ and have some variation\n",
        "    # of .no-act.order or .safetensors in their HF repo.\n",
        "    print(f\"{datetime.now()} - Using AutoGPTQForCausalLM for quantized models\")\n",
        "\n",
        "    if \".safetensors\" in model_basename:\n",
        "        # Remove the \".safetensors\" ending if present\n",
        "        model_basename = model_basename.replace(\".safetensors\", \"\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    print(f\"{datetime.now()} - Tokenizer loaded\")\n",
        "\n",
        "    model = AutoGPTQForCausalLM.from_quantized(\n",
        "        model_id,\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        use_triton=False,\n",
        "        quantize_config=None,\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def load_full_model(model_id, model_basename, device_type, logging):\n",
        "    \"\"\"\n",
        "    Load a full model using either LlamaTokenizer or AutoModelForCausalLM.\n",
        "\n",
        "    This function loads a full model based on the specified device type.\n",
        "    If the device type is 'mps' or 'cpu', it uses LlamaTokenizer and LlamaForCausalLM.\n",
        "    Otherwise, it uses AutoModelForCausalLM.\n",
        "\n",
        "    Parameters:\n",
        "    - model_id (str): The identifier for the model on HuggingFace Hub.\n",
        "    - model_basename (str): The base name of the model file.\n",
        "    - device_type (str): The type of device where the model will run.\n",
        "    - logging (logging.Logger): Logger instance for logging messages.\n",
        "\n",
        "    Returns:\n",
        "    - model (Union[LlamaForCausalLM, AutoModelForCausalLM]): The loaded model.\n",
        "    - tokenizer (Union[LlamaTokenizer, AutoTokenizer]): The tokenizer associated with the model.\n",
        "\n",
        "    Notes:\n",
        "    - The function uses the `from_pretrained` method to load both the model and the tokenizer.\n",
        "    - Additional settings are provided for NVIDIA GPUs, such as loading in 4-bit and setting the compute dtype.\n",
        "    \"\"\"\n",
        "\n",
        "    if device_type.lower() in [\"mps\", \"cpu\"]:\n",
        "        print(f\"{datetime.now()} - Using LlamaTokenizer\")\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(model_id, cache_dir=\"data/models/\")\n",
        "        model = LlamaForCausalLM.from_pretrained(model_id, cache_dir=\"data/models/\")\n",
        "    else:\n",
        "        print(f\"{datetime.now()} - Using AutoModelForCausalLM for full models\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"data/models/\")\n",
        "        print(f\"{datetime.now()} - Tokenizer loaded\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            cache_dir=MODELS_PATH,\n",
        "            trust_remote_code=True, # set these if you are using NVIDIA GPU\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            max_memory={0: \"15GB\"} # Uncomment this line with you encounter CUDA out of memory errors\n",
        "        )\n",
        "        model.tie_weights()\n",
        "    return model, tokenizer\n",
        "\n",
        "def load_quantized_model_awq(model_id, logging):\n",
        "    \"\"\"\n",
        "    Load a AWQ quantized model using AutoModelForCausalLM.\n",
        "\n",
        "    This function loads a quantized model that ends with AWQ.\n",
        "\n",
        "    Parameters:\n",
        "    - model_id (str): The identifier for the model on HuggingFace Hub.\n",
        "    - logging (logging.Logger): Logger instance for logging messages.\n",
        "\n",
        "    Returns:\n",
        "    - model (AutoModelForCausalLM): The loaded quantized model.\n",
        "    - tokenizer (AutoTokenizer): The tokenizer associated with the model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The code supports all huggingface models that ends with AWQ.\n",
        "    print(f\"{datetime.now()} - Using AutoModelForCausalLM for AWQ quantized models\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    print(f\"{datetime.now()} - Tokenizer loaded\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    return model, tokenizer\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}