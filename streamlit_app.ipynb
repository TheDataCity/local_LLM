{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import click\n",
        "import torch\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from run_localLLM import run_main_process\n",
        "from run_localLLM import chatWithLLM\n",
        "\n",
        "device_type = \"cpu\"\n",
        "show_sources = False \n",
        "use_history = False \n",
        "model_type = \"llama\" \n",
        "save_qa = False \n",
        "\n",
        "st.set_page_config(page_title=\"Companies Information Bot\", page_icon=\"\ud83c\udf10\")\n",
        "st.title(\"\ud83d\udcac Companies Info Chat Bot\")\n",
        "\n",
        "# Setup memory for contextual conversation\n",
        "msgs = StreamlitChatMessageHistory()\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", chat_memory=msgs, return_messages=True)\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")\n",
        "\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = \"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "        self.run_id_ignore_token = None\n",
        "\n",
        "    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):\n",
        "        # Workaround to prevent showing the rephrased question as output\n",
        "        if prompts[0].startswith(\"Human\"):\n",
        "            self.run_id_ignore_token = kwargs.get(\"run_id\")\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        if self.run_id_ignore_token == kwargs.get(\"run_id\", False):\n",
        "            return\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "class PrintRetrievalHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container):\n",
        "        self.status = container.status(\"Company Information Context\")\n",
        "\n",
        "    def on_retriever_start(self, serialized: dict, query: str, **kwargs):\n",
        "        print(\"Query :- \", query)\n",
        "        #self.status.write(f\"**Question:** {query}\")\n",
        "        #self.status.update(label=f\"**Context Retrieval:** {query}\")\n",
        "        self.status.update(label=f\"Company Information Context\")\n",
        "\n",
        "    def on_retriever_end(self, documents, **kwargs):\n",
        "        for idx, doc in enumerate(documents):\n",
        "            source = os.path.basename(doc.metadata[\"source\"])\n",
        "            self.status.write(f\"**Document {idx+1} from {source}**\")\n",
        "            self.status.markdown(doc.page_content)\n",
        "        self.status.update(state=\"complete\")\n",
        "\n",
        "\n",
        "def clear_content():\n",
        "    msgs.clear()\n",
        "    initialMessage = 'Ask me anything about \"' + st.session_state['issue'] + '\" !'\n",
        "    msgs.add_ai_message(initialMessage)\n",
        "\n",
        "with st.sidebar:\n",
        "    # Dropdown Menu\n",
        "    option = st.selectbox(\n",
        "      'Select a company about which you want to know',\n",
        "      (pd.read_csv(f'data/Company_Names_with_Company_Numbers(Options).csv')),index=None, placeholder=\"Select a company\",on_change=clear_content, key = 'issue')\n",
        "\n",
        "    # st.write('You selected:', option)\n",
        "    \"[View the source code](https://github.com/TheDataCity/local_LLM)\"\n",
        "    # if tempOption != option:\n",
        "    #     checkFlag = True\n",
        "    #     tempOption = option\n",
        "\n",
        "    show_sources = st.checkbox('Show the Sources')\n",
        "    use_history = st.checkbox('Use the History')\n",
        "    save_qa = st.checkbox('Save the Chat') \n",
        "\n",
        "    device_type = st.selectbox(\n",
        "      'Select a device type',\n",
        "      (\"cpu\",\"cuda\",\"ipu\",\"xpu\",\"mkldnn\",\"opengl\",\"opencl\",\"ideep\",\"hip\",\"ve\",\"fpga\",\"ort\",\"xla\",\"lazy\",\"vulkan\",\"mps\",\"meta\",\"hpu\",\"mtia\")\n",
        "      ,index=None, placeholder=\"Select a device\")\n",
        "    \n",
        "    model_type = st.selectbox(\n",
        "      'Select a model type',\n",
        "      (\"llama\", \"mistral\", \"non_llama\")\n",
        "      ,index=None, placeholder=\"Select a LLM model\")\n",
        "\n",
        "    if option is not None:\n",
        "        last_parenthesis_pos = option.rfind('(')\n",
        "        # Extracting the text inside the last set of parentheses\n",
        "        if last_parenthesis_pos != -1:\n",
        "            companyNumber = st.session_state['issue'][last_parenthesis_pos + 1:-1]  # Slicing from '(' to the end, excluding ')'\n",
        "        else:\n",
        "            companyNumber = \"\"\n",
        "    \n",
        "        qa = run_main_process(device_type,show_sources,use_history,model_type,save_qa, company_number=companyNumber)\n",
        "        chackFlag = False\n",
        "\n",
        "if not option:\n",
        "        st.info(\"Please select a company from the drop down menu\")\n",
        "        st.stop()\n",
        "\n",
        "avatars = {\"human\": \"\u2753\", \"ai\": \"\u2744\ufe0f\"}\n",
        "for msg in msgs.messages:\n",
        "    st.chat_message(avatars[msg.type]).write(msg.content)\n",
        "\n",
        "if user_query := st.chat_input(placeholder=\"Ask me anything about the company!\"):\n",
        "    st.chat_message(\"user\", avatar= \"\u2753\").write(user_query)\n",
        "\n",
        "    with st.chat_message(\"assistant\", avatar=\"\u2744\ufe0f\"):\n",
        "        print(\"user_query :- \", user_query)\n",
        "        response, sources = chatWithLLM(show_sources, save_qa, qa, user_query)\n",
        "        st.write(response)\n",
        "        print(sources)\n",
        "        # response = qa_chain.run(user_query, callbacks=[retrieval_handler, stream_handler])\n",
        "        # st.json(jsonResponse)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}