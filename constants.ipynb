{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "from chromadb.config import Settings\n",
        "\n",
        "from langchain.document_loaders import CSVLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\n",
        "from langchain.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader\n",
        "\n",
        "\n",
        "# load_dotenv()\n",
        "ROOT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))\n",
        "\n",
        "# Define the folder for storing database\n",
        "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/data/text_file_database\"\n",
        "\n",
        "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/data/vector_database\"\n",
        "\n",
        "MODELS_PATH = f\"{ROOT_DIRECTORY}/data/models\"\n",
        "\n",
        "# Can be changed to a specific number\n",
        "INGEST_THREADS = os.cpu_count() or 8\n",
        "\n",
        "# Define the Chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    is_persistent=True,\n",
        ")\n",
        "\n",
        "# Context Window and Max New Tokens\n",
        "CONTEXT_WINDOW_SIZE = 4096\n",
        "MAX_NEW_TOKENS = CONTEXT_WINDOW_SIZE  # int(CONTEXT_WINDOW_SIZE/4)\n",
        "\n",
        "#### If you get a \"not enough space in the buffer\" error, you should reduce the values below, start with half of the original values and keep halving the value until the error stops appearing\n",
        "\n",
        "N_GPU_LAYERS = 100  # Llama-2-70B has 83 layers\n",
        "N_BATCH = 512\n",
        "\n",
        "### From experimenting with the Llama-2-7B-Chat-GGML model on 8GB VRAM, these values work:\n",
        "# N_GPU_LAYERS = 20\n",
        "# N_BATCH = 512\n",
        "\n",
        "DOCUMENT_MAP = {\n",
        "    \".txt\": TextLoader,\n",
        "    \".md\": UnstructuredMarkdownLoader,\n",
        "    \".py\": TextLoader,\n",
        "    # \".pdf\": PDFMinerLoader,\n",
        "    \".pdf\": UnstructuredFileLoader,\n",
        "    \".csv\": CSVLoader,\n",
        "    \".xls\": UnstructuredExcelLoader,\n",
        "    \".xlsx\": UnstructuredExcelLoader,\n",
        "    \".docx\": Docx2txtLoader,\n",
        "    \".doc\": Docx2txtLoader,\n",
        "}\n",
        "\n",
        "# Default Instructor Model\n",
        "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"  # Uses 1.5 GB of VRAM (High Accuracy with lower VRAM usage)\n",
        "\n",
        "####\n",
        "#### OTHER EMBEDDING MODEL OPTIONS\n",
        "####\n",
        "\n",
        "# EMBEDDING_MODEL_NAME = \"hkunlp/instructor-xl\" # Uses 5 GB of VRAM (Most Accurate of all models)\n",
        "# EMBEDDING_MODEL_NAME = \"intfloat/e5-large-v2\" # Uses 1.5 GB of VRAM (A little less accurate than instructor-large)\n",
        "# EMBEDDING_MODEL_NAME = \"intfloat/e5-base-v2\" # Uses 0.5 GB of VRAM (A good model for lower VRAM GPUs)\n",
        "# EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\" # Uses 0.2 GB of VRAM (Less accurate but fastest - only requires 150mb of vram)\n",
        "\n",
        "####\n",
        "#### MULTILINGUAL EMBEDDING MODELS\n",
        "####\n",
        "\n",
        "# EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large\" # Uses 2.5 GB of VRAM\n",
        "# EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-base\" # Uses 1.2 GB of VRAM\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#### SELECT AN OPEN SOURCE LLM (LARGE LANGUAGE MODEL)\n",
        "# Select the Model ID and model_basename\n",
        "# load the LLM for generating Natural Language responses\n",
        "\n",
        "#### GPU VRAM Memory required for LLM Models (ONLY) by Billion Parameter value (B Model)\n",
        "#### Does not include VRAM used by Embedding Models - which use an additional 2GB-7GB of VRAM depending on the model.\n",
        "####\n",
        "#### (B Model)   (float32)    (float16)    (GPTQ 8bit)         (GPTQ 4bit)\n",
        "####    7b         28 GB        14 GB       7 GB - 9 GB        3.5 GB - 5 GB\n",
        "####    13b        52 GB        26 GB       13 GB - 15 GB      6.5 GB - 8 GB\n",
        "####    32b        130 GB       65 GB       32.5 GB - 35 GB    16.25 GB - 19 GB\n",
        "####    65b        260.8 GB     130.4 GB    65.2 GB - 67 GB    32.6 GB -  - 35 GB\n",
        "\n",
        "# MODEL_ID = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "# MODEL_BASENAME = \"llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
        "\n",
        "####\n",
        "#### (FOR GGUF MODELS)\n",
        "####\n",
        "\n",
        "# MODEL_ID = \"TheBloke/Llama-2-13b-Chat-GGUF\"\n",
        "# MODEL_BASENAME = \"llama-2-13b-chat.Q4_K_M.gguf\"\n",
        "\n",
        "MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
        "MODEL_BASENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "\n",
        "# MODEL_ID = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "# MODEL_BASENAME = \"mistral-7b-instruct-v0.1.Q8_0.gguf\"\n",
        "\n",
        "# MODEL_ID = \"TheBloke/Llama-2-70b-Chat-GGUF\"\n",
        "# MODEL_BASENAME = \"llama-2-70b-chat.Q4_K_M.gguf\"\n",
        "\n",
        "####\n",
        "#### (FOR HF MODELS)\n",
        "####\n",
        "\n",
        "# MODEL_ID = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "# MODEL_BASENAME = None\n",
        "# MODEL_ID = \"TheBloke/vicuna-7B-1.1-HF\"\n",
        "# MODEL_BASENAME = None\n",
        "# MODEL_ID = \"TheBloke/Wizard-Vicuna-7B-Uncensored-HF\"\n",
        "# MODEL_ID = \"TheBloke/guanaco-7B-HF\"\n",
        "# MODEL_ID = 'NousResearch/Nous-Hermes-13b' # Requires ~ 23GB VRAM. Using STransformers\n",
        "# alongside will 100% create OOM on 24GB cards.\n",
        "# llm = load_model(device_type, model_id=model_id)\n",
        "\n",
        "####\n",
        "#### (FOR GPTQ QUANTIZED) Select a llm model based on your GPU and VRAM GB. Does not include Embedding Models VRAM usage.\n",
        "####\n",
        "\n",
        "##### 48GB VRAM Graphics Cards (RTX 6000, RTX A6000 and other 48GB VRAM GPUs) #####\n",
        "\n",
        "### 65b GPTQ LLM Models for 48GB GPUs (*** With best embedding model: hkunlp/instructor-xl ***)\n",
        "# MODEL_ID = \"TheBloke/guanaco-65B-GPTQ\"\n",
        "# MODEL_BASENAME = \"model.safetensors\"\n",
        "# MODEL_ID = \"TheBloke/Airoboros-65B-GPT4-2.0-GPTQ\"\n",
        "# MODEL_BASENAME = \"model.safetensors\"\n",
        "# MODEL_ID = \"TheBloke/gpt4-alpaca-lora_mlp-65B-GPTQ\"\n",
        "# MODEL_BASENAME = \"model.safetensors\"\n",
        "# MODEL_ID = \"TheBloke/Upstage-Llama1-65B-Instruct-GPTQ\"\n",
        "# MODEL_BASENAME = \"model.safetensors\"\n",
        "\n",
        "##### 24GB VRAM Graphics Cards (RTX 3090 - RTX 4090 (35% Faster) - RTX A5000 - RTX A5500) #####\n",
        "\n",
        "### 13b GPTQ Models for 24GB GPUs (*** With best embedding model: hkunlp/instructor-xl ***)\n",
        "# MODEL_ID = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
        "# MODEL_BASENAME = \"Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n",
        "# MODEL_ID = \"TheBloke/vicuna-13B-v1.5-GPTQ\"\n",
        "# MODEL_BASENAME = \"model.safetensors\"\n",
        "# MODEL_ID = \"TheBloke/Nous-Hermes-13B-GPTQ\"\n",
        "# MODEL_BASENAME = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\n",
        "# MODEL_ID = \"TheBloke/WizardLM-13B-V1.2-GPTQ\"\n",
        "# MODEL_BASENAME = \"gptq_model-4bit-128g.safetensors\n",
        "\n",
        "### 30b GPTQ Models for 24GB GPUs (*** Requires using intfloat/e5-base-v2 instead of hkunlp/instructor-large as embedding model ***)\n",
        "# MODEL_ID = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n",
        "# MODEL_BASENAME = \"Wizard-Vicuna-30B-Uncensored-GPTQ-4bit--1g.act.order.safetensors\"\n",
        "# MODEL_ID = \"TheBloke/WizardLM-30B-Uncensored-GPTQ\"\n",
        "# MODEL_BASENAME = \"WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors\"\n",
        "\n",
        "##### 8-10GB VRAM Graphics Cards (RTX 3080 - RTX 3080 Ti - RTX 3070 Ti - 3060 Ti - RTX 2000 Series, Quadro RTX 4000, 5000, 6000) #####\n",
        "### (*** Requires using intfloat/e5-small-v2 instead of hkunlp/instructor-large as embedding model ***)\n",
        "\n",
        "### 7b GPTQ Models for 8GB GPUs\n",
        "# MODEL_ID = \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\"\n",
        "# MODEL_BASENAME = \"Wizard-Vicuna-7B-Uncensored-GPTQ-4bit-128g.no-act.order.safetensors\"\n",
        "# MODEL_ID = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n",
        "# MODEL_BASENAME = \"WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n",
        "# MODEL_ID = \"TheBloke/wizardLM-7B-GPTQ\"\n",
        "# MODEL_BASENAME = \"wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors\"\n",
        "\n",
        "####\n",
        "#### (FOR GGML) (Quantized cpu+gpu+mps) models - check if they support llama.cpp\n",
        "####\n",
        "\n",
        "# MODEL_ID = \"TheBloke/wizard-vicuna-13B-GGML\"\n",
        "# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q4_0.bin\"\n",
        "# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q6_K.bin\"\n",
        "# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q2_K.bin\"\n",
        "# MODEL_ID = \"TheBloke/orca_mini_3B-GGML\"\n",
        "# MODEL_BASENAME = \"orca-mini-3b.ggmlv3.q4_0.bin\"\n",
        "\n",
        "####\n",
        "#### (FOR AWQ QUANTIZED) Select a llm model based on your GPU and VRAM GB. Does not include Embedding Models VRAM usage.\n",
        "### (*** MODEL_BASENAME is not actually used but have to contain .awq so the correct model loading is used ***)\n",
        "### (*** Compute capability 7.5 (sm75) and CUDA Toolkit 11.8+ are required ***)\n",
        "####\n",
        "# MODEL_ID = \"TheBloke/Llama-2-7B-Chat-AWQ\"\n",
        "# MODEL_BASENAME = \"model.safetensors.awq\"\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}